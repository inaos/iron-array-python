{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reductions\n",
    "\n",
    "ironArray supports a broad range of reduction facilities, like `sum`, `min`, `max`, `mean` and others.  Also, they work on any (or group of) dimensions.  One interesting aspect of these is that the implementation leverages the multi-threading capabilities of ironArray, so they can be pretty fast (although some times they need some help from the user).\n",
    "\n",
    "In order to exercise some of this functionality, let's use the precipitation data from a period of 3 months.  In case the data has not been downloaded yet, this should not take more than a couple of minutes.  Let's go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import iarray as ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset precip-3m.iarr is already here!\n"
     ]
    }
   ],
   "source": [
    "%run fetch_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333M\tprecip1.iarr\n",
      "280M\tprecip2.iarr\n",
      "268M\tprecip3.iarr\n",
      "788M\tprecip-3m.iarr\n",
      "966M\tprecip-3m-optimal.iarr\n",
      "363M\tprecip1.zarr\n",
      "357M\tprecip2.zarr\n",
      "355M\tprecip3.zarr\n",
      "1,1G\tprecip-3m.zarr\n"
     ]
    }
   ],
   "source": [
    "!du -sh precip*.iarr\n",
    "!du -sh precip*.zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole dataset is stored now on a single file of less than 1 GB, which is about 10x less than the original dataset thanks to compression.  That's a big win!  In addition there is an assortment of other, smaller files for the purposes of tutorials.  Also, and for comparison purposes, you will find the datasets in Zarr format, another popular array format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ok.  Now, let's import this data into ironArray before proceeding with reductions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IArray (3, 720, 721, 1440) np.float32>\n",
      "cratio:  13.17\n",
      "CPU times: user 68.5 ms, sys: 383 ms, total: 451 ms\n",
      "Wall time: 409 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ia_precip = ia.load(\"precip-3m.iarr\")\n",
    "print(ia_precip)\n",
    "print(\"cratio: \", round(ia_precip.cratio, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ok, so ironArray achieves a compression ratio of more than 10x, which is a big win in terms of memory consumption.  Now, let's have a look at how reduction works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 51s, sys: 987 ms, total: 1min 52s\n",
      "Wall time: 5.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduc0 = ia.mean(ia_precip, axis=(0, 2, 3)).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ok, so that's pretty slow.  Now, it is time to remember that ironArray uses chunked storage, even when it holds data in-memory.  In this case, we have been traversing the array in a very innefficient way.  In general, in chunked storage, it is always better to start reducing by the dimension that is the largest, and we took the inverse order.  With this in mind, let's try with a more reasonable order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.3 s, sys: 198 ms, total: 8.5 s\n",
      "Wall time: 672 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduc0 = ia.mean(ia_precip, axis=(3, 2, 0)).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ok, that's much better.  Now, for the matter of comparsion, let's do the same computation with NumPy.  Let's first get the NumPy array from the ironArray one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 s, sys: 1.85 s, total: 12.5 s\n",
      "Wall time: 4.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "precip = ia_precip.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "and now the actual reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 504 ms, sys: 0 ns, total: 504 ms\n",
      "Wall time: 504 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduc1 = np.mean(precip, axis=(3, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(reduc0, reduc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So, by default, ironArray provides good enough performance out of the box.  If you need to compute this sort of reductions lots of times, you may want to fine tune your ironArray array parameters.  If so, just keep reading.  But before proceeding further, let's save our data for later reuse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optimization Tips\n",
    "\n",
    "As we know, most of ironArray optimization stems from its two levels of partitioning.  Previously, we have been using automatic values for chunks and blocks (based on CPU's cache sizes).  But for maximum speed, there is no replacement for fine tuning chunk and block shapes manually.\n",
    "\n",
    "Let's start by loading the array that we previously stored on-disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 23.7 ms, total: 23.7 ms\n",
      "Wall time: 22.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import iarray as ia\n",
    "ia_precip = ia.open(\"precip-3m.iarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some milliseconds is not too much for loading a 9 GB large array from disk.  It turns out that `open()` just loads data when it needs it by default (but for a full in-memory load, see the `load()` function).  So, only a tiny portion of the file (the metadata) is read in order to figure out how to access the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's start with a first attempt to optimize the speed of the reduction by favoring it when creating another container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96.3 ms, sys: 334 ms, total: 431 ms\n",
      "Wall time: 390 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ia.set_config(favor=ia.Favors.SPEED)\n",
    "new_precip = ia_precip.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And let's try the reduction with the new array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.18 s, sys: 203 ms, total: 8.38 s\n",
      "Wall time: 663 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduc1 = ia.mean(new_precip, axis=(3, 2, 0)).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Well, we did not had a lot of improvement here, but in general you should allways try to use the `favor=ia.Favors.SPEED` setting when looking for speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Another path for improvement is to fine tune the chunk and block shapes. Let's see the current chunk and block shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 128, 256)\n",
      "(1, 16, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "print(ia_precip.chunks)\n",
    "print(ia_precip.blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After several iterations trying different chunk shapes and block shapes, one can reach a sort of optimal configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.7 s, sys: 4.41 s, total: 18.1 s\n",
      "Wall time: 8.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with ia.config(chunks=(1, 360, 128, 1440), blocks=(1, 8, 8, 720)):\n",
    "    new_precip = ia_precip.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.22 s, sys: 23.9 ms, total: 5.25 s\n",
      "Wall time: 326 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduc1 = ia.mean(new_precip, axis=(3, 2, 0)).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So, this new time is quite good (somewhat faster than NumPy actually).  That means that, with a bit of manual tuning, you can end with quite fast computations with ironArray.  And most specially, using (potentially much) less memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expressions with on-disk operands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been playing with expressions on in-memory operands, but ironArray can seamlessly work with on-disk arrays too.  Let's save the previous array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.54 ms, sys: 0 ns, total: 1.54 ms\n",
      "Wall time: 1.47 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ia.save(\"precip-3m-optimal.iarr\", new_precip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "and let's re-open again but in 'lazy' mode and do the reduction from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IArray (3, 720, 721, 1440) np.float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_precip_opt = ia.open(\"precip-3m-optimal.iarr\")\n",
    "new_precip_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.68 s, sys: 994 ms, total: 6.67 s\n",
      "Wall time: 806 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduc1 = ia.mean(new_precip_opt, axis=(3, 2, 0)).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So, although this on-disk time is considerable slower than the in-memory one, it is still not bad at all.  Of course, the filesystem cache is probably saving quite a bit of time, although modern SSDs can improve speeds quite significantly.  This is interesting because that means that doing out-of-core operations will consume far less memory, while still keeping reasonable times.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}