{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expression Evaluation (User Defined Functions)\n",
    "\n",
    "So far we have seen that ironArray has support for evaluating expressions that are passed as strings or as simple Python statements.  There is another, more flexible way for evaluating expressions called User Defined Functions, or UDFs for short.\n",
    "\n",
    "UDFs are small functions that can be expressed in a simple subset of Python.  These functions are then passed to the internal LLVM compiler in ironArray and a binary specific and optimized for the local machine is generated.  This binary is optimized for the CPU and in addition, it will make use of the [Intel SVML library](https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/intrinsics/intrinsics-for-short-vector-math-library-operations/overview-intrinsics-for-short-vector-math-library-svml-functions.html) for accelerating the evaluation of transcendental functions.\n",
    "\n",
    "Let's see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext memprofiler\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import iarray as ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 103 ms, sys: 203 ms, total: 306 ms\n",
      "Wall time: 306 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "precip1 = ia.load(\"precip1.iarr\")\n",
    "precip2 = ia.load(\"precip2.iarr\")\n",
    "precip3 = ia.load(\"precip3.iarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's define a simple function that computes the mean for this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from iarray.udf import jit, Array, float32, float64\n",
    "\n",
    "@jit()\n",
    "def mean(out: Array(float32, 3),\n",
    "         p1: Array(float32, 3),\n",
    "         p2: Array(float32, 3),\n",
    "         p3: Array(float32, 3)) -> int:\n",
    "\n",
    "    l = out.shape[0]\n",
    "    m = out.shape[1]\n",
    "    n = out.shape[2]\n",
    "\n",
    "    for i in range(l):\n",
    "        for j in range(m):\n",
    "            for k in range(n):\n",
    "                value = p1[i,j,k] + p2[i,j,k] + p3[i,j,k]\n",
    "                out[i,j,k] = value / 3\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is known as a _User Defined Function_ (_UDF_ for short) and it has a syntax that is a small subset of Python.  The order of the parameters passed matters: first comes the output and then a variable number of inputs (3 in this case).  The type of the parameters is always an `Array`, where you specify the data type (currently `float32` or `float64`), and the dimensions (3 in this case).\n",
    "\n",
    "Finally, you can make use of the `shape` attribute of the parameters so as to access the _window_ to which the UDF will apply.  It is important to have in mind that the parameters do not have access to whole arrays passed to the function, but only to one part (also known as _window_).  Later we will see an example where we will work more explictly with windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the ironArray expression from this User Defined Function with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.8 ms, sys: 4.84 ms, total: 27.7 ms\n",
      "Wall time: 27.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "precip_expr = ia.expr_from_udf(mean, [precip1, precip2, precip3], btune=False, favor=ia.Favors.BALANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As can be seen, converting the user defined function into a native ironArray expression is pretty fast.  And as always, in order to do the actual evaluation, we have to call `.eval()` on the expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IArray (720, 721, 1440) np.float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memprofiler: used 495.11 MiB RAM (peak of 495.11 MiB) in 0.7460 s, total RAM usage 1381.53 MiB\n"
     ]
    }
   ],
   "source": [
    "%%mprof_run iarray-mean\n",
    "precip_mean = precip_expr.eval()\n",
    "precip_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this time with the evaluation via a regular lazy expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_expr2 = (precip1 + precip2 + precip3) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IArray (720, 721, 1440) np.float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memprofiler: used 393.89 MiB RAM (peak of 395.06 MiB) in 0.7511 s, total RAM usage 1775.44 MiB\n"
     ]
    }
   ],
   "source": [
    "%%mprof_run iarray-mean2\n",
    "precip_mean2 = precip_expr2.eval()\n",
    "precip_mean2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the times and memory consumption are very close.  It turns out that UDFs compile and execute in ironArray make use the very same LLVM machinery, which explains times being similar.  It is up to you to decide to use one or the other depending on your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "An User Defined Function also have access to a good assortment of math functions, and you can access them via the usual Python `math` module. Let's see an example for our dataset, and although this does not make much sense for precipitation data, we can use this as an indication of the efficiency of the computational engine inside ironArray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "@jit()\n",
    "def trans(out: Array(float32, 3),\n",
    "          p1: Array(float32, 3),\n",
    "          p2: Array(float32, 3),\n",
    "          p3: Array(float32, 3)) -> int:\n",
    "\n",
    "    l = out.shape[0]\n",
    "    m = out.shape[1]\n",
    "    n = out.shape[2]\n",
    "\n",
    "    for i in range(l):\n",
    "        for j in range(m):\n",
    "            for k in range(n):\n",
    "                value = math.sin(p1[i,j,k]) * math.sin(p2[i,j,k]) + math.cos(p2[i,j,k])\n",
    "                value *= math.tan(p1[i,j,k])\n",
    "                value += math.sqrt(p3[i,j,k]) * 2\n",
    "                out[i,j,k] = value\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 ms, sys: 4 ms, total: 24 ms\n",
      "Wall time: 23.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "precip_expr = ia.expr_from_udf(trans, [precip1, precip2, precip3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IArray (720, 721, 1440) np.float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memprofiler: used 651.00 MiB RAM (peak of 651.00 MiB) in 1.1034 s, total RAM usage 2427.06 MiB\n"
     ]
    }
   ],
   "source": [
    "%%mprof_run iarray-trans\n",
    "precip_mean = precip_expr.eval()\n",
    "precip_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this case we see that the overhead of using transcendental functions is pretty the same than plain arithmetic operations (sum, rest, mult, division...).  This is a very significant fact because traditionally transcendental functions took really long time compared with plain arithmetic; not anymore thanks to the combination of LLVML and Intel SVML.  The good mix between compiler optimization (via LLVM) and SIMD usage (via SVML) makes this couple shine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of comparison, let's compute the same expression with NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 s, sys: 2 s, total: 12.2 s\n",
      "Wall time: 4.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p1_ = precip1.data\n",
    "p2_ = precip2.data\n",
    "p3_ = precip3.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memprofiler: used 2852.01 MiB RAM (peak of 8555.00 MiB) in 8.3983 s, total RAM usage 13849.50 MiB\n"
     ]
    }
   ],
   "source": [
    "%%mprof_run np_trans\n",
    "np_result = (np.tan(p1_) * (np.sin(p1_) * np.sin(p2_) + np.cos(p2_)) + np.sqrt(p3_) * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is really slow, but this is kind of expected because NumPy does not have support for SVML or multithreading (at this time at least), and we all know that transcendental functions always took quite a lot to execute on a regular CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with data windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already mentioned that User Defined Functions can access only a part (a window) of the dataset.  Here we will have a more in deep look at how this works and how to squeeze all the functionality out of it.\n",
    "\n",
    "Let's start by creating an zeroed array, and let's populate it with a UDF so that we get a diagonal array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ia.zeros((10, 10), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UDF looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def fill_diag(out: Array(float64, 2), in1: Array(float64, 2)) -> int:\n",
    "    n = out.shape[0]\n",
    "    m = out.shape[1]\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            out[i, j] = 1. if i == j else in1[i, j]\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_expr = ia.expr_from_udf(fill_diag, [e], chunks=(5, 5), blocks=(3, 3))\n",
    "result = fill_expr.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 1. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 1. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, instead of getting a diagonal array, we get a lot of non-zeros outside of the diagonal.  What we are seeing here is some internal details of how expressions are evaluated inside ironArray.  The evaluation engine splits the inputs in blocks (also called **data windows** in this context), so that the engine can evaluate them in parallel.  So `out.shape` in the UDF above refers to just this data window.\n",
    "\n",
    "In fact, if you look at the pattern above, you can see small 3x3 arrays (the size of blocks, or data windows) that are diagonal.  Incidentally, we don't see a patchwork made of full 3x3 arrays because the chunks are 5x5, so the data windows also end when a chunk ends.  Take some time to recognize the patterns because this will allow you to get a nice picture of how data is arranged during evaluations.\n",
    "\n",
    "Now, let's go back to our original goal of getting a diagonal array.  Have a look at this new UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def fill_diag2(out: Array(float64, 2), in1: Array(float64, 2)) -> int:\n",
    "    n = out.shape[0]\n",
    "    m = out.shape[1]\n",
    "    start_n = out.window_start[0]\n",
    "    start_m = out.window_start[1]\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            out[i, j] = 1. if i == j and start_n == start_m else in1[i, j]\n",
    "            # another way\n",
    "            # out[i, j] = 1. if i + start_n == j + start_m else in1[i, j]\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_expr2 = ia.expr_from_udf(fill_diag2, [e], chunks=(5, 5), blocks=(3, 3))\n",
    "result = fill_expr2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's much better.  Let's recap what the UDF is doing now.  First, it is getting the starting points for each data window via the `out.window_start` property.  Second, we are enforcing that we want to add ones only in the data windows that lay the diagonal, and not outside.\n",
    "\n",
    "In general, you should keep in mind that you can access any element in operands (or result) arrays as long as it is inside the current data window (the block).  Although initially this might be a bit confusing, this 'feature' enables performing expressions in parallel (i.e. one thread evaluates its corresponding window).  Combine this with SVML/LLVM and you will get the principles on why ironArray can be so fast in evaluating UDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
